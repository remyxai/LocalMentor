{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LLM Evaluation via Skills Assessment\n",
        "\n",
        "Using a modified [FLASK method](https://arxiv.org/pdf/2307.10928.pdf), this notebook helps to benchmark and evaluate LLMs for your application.\n",
        "\n",
        "With a fine-grained skills assessment and scoring LLM responses on a 1-5 likert scale, researchers found strong correlation between human and LLM evaluators.\n",
        "\n",
        "This example compares [OpenAI's GPT-4](https://openai.com/research/gpt-4) the [LocalMentor python package](https://github.com/remyxai/LocalMentor)."
      ],
      "metadata": {
        "id": "VerUWxvrf-p9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "You will need an OpenAI token to complete the evaluation. If you run into import issues, please restart the runtime.\n",
        "\n",
        "You can run the optional cell to prevent the notebook from timing out in case of long runs."
      ],
      "metadata": {
        "id": "JBqkedxiq6qh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br2VOg3cES87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828a423b-d02d-4cf3-f543-51caacceccf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter token here··········\n"
          ]
        }
      ],
      "source": [
        "# @title Set OpenAI token\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "token = getpass('Enter token here')\n",
        "os.environ['OPENAI_API_KEY'] = token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (Optional) Execute for long running processes\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "llOOVdoKQVT3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "129ca44c-c1dd-4364-94b0-5c9a7ca5a8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YyNapHN5jbl",
        "outputId": "a3c6613c-6dba-4210-f42e-ce2655dc1b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q localmentor\n",
        "!pip install -q guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmJyFrtFEPrk"
      },
      "outputs": [],
      "source": [
        "from localmentor import mentor\n",
        "import guidance\n",
        "from guidance import models, gen, system, user, assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We've sampled 10 prompts for evaluating each model for faster results. You can try adding more prompts for greater confidence."
      ],
      "metadata": {
        "id": "9QN0u5jNrieL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Choose prompts for evaluation\n",
        "prompt_1 = \"What are the key factors to consider when starting a new business?\" # @param {type:\"string\"}\n",
        "prompt_2 = \"What are the most common mistakes startups make and how can they be avoided?\" # @param {type:\"string\"}\n",
        "prompt_3 = \"What are the best strategies for raising initial capital for a startup?\" # @param {type:\"string\"}\n",
        "prompt_4 = \"How can a startup effectively validate its business idea?\" # @param {type:\"string\"}\n",
        "prompt_5 = \"What are the best practices for managing cash flow in a startup?\" # @param {type:\"string\"}\n",
        "prompt_6 = \"How can a startup create a strong and sustainable company culture?\" # @param {type:\"string\"}\n",
        "prompt_7 = \"What are the key metrics a startup should track and why?\" # @param {type:\"string\"}\n",
        "prompt_8 = \"How can a startup effectively market its product or service?\" # @param {type:\"string\"}\n",
        "prompt_9 = \"What are the best strategies for a startup to handle competition?\" # @param {type:\"string\"}\n",
        "prompt_10 = \"How can a startup maintain its focus and avoid distractions?\" # @param {type:\"string\"}\n",
        "\n",
        "prompts = [\n",
        "    prompt_1,\n",
        "    prompt_2,\n",
        "    prompt_3,\n",
        "    prompt_4,\n",
        "    prompt_5,\n",
        "    prompt_6,\n",
        "    prompt_7,\n",
        "    prompt_8,\n",
        "    prompt_9,\n",
        "    prompt_10\n",
        "    ]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yvsJRhcHT0XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions\n",
        "def guidance_helper(system_prompt, model, prompt, output_key, max_tokens=1500):\n",
        "  mdl = model\n",
        "  with system():\n",
        "    mdl += system_prompt\n",
        "  with user():\n",
        "    mdl += prompt\n",
        "  with assistant():\n",
        "    mdl += gen(name=output_key, max_tokens=max_tokens)\n",
        "  return mdl[output_key]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_VhgatXOyZG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0keWVja7SBd",
        "outputId": "f3dbc9a6-6ab3-4e84-83fa-1ddbc13b9424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading mentor...\n"
          ]
        }
      ],
      "source": [
        "# @title Generate a response per model\n",
        "from ast import literal_eval\n",
        "\n",
        "system_prompt = \"Provide a comprehensive and detailed response that includes innovative insights and clear, structured explanations. Focus on delivering practical, actionable advice. Ensure relevance to the startup and tech industry, emphasizing current trends and future possibilities. Be specific in your examples and explanations, and consider adding unique perspectives or novel approaches to standard practices. Aim for clarity in communication, organizing your response logically and coherently.\\n\"\n",
        "\n",
        "responses = []\n",
        "for prompt in prompts:\n",
        "    advice = mentor(system_prompt + prompt)\n",
        "\n",
        "    oai_advice = guidance_helper(system_prompt, models.OpenAI(\"gpt-4\"), prompt, \"advice\")\n",
        "    responses.append((advice, oai_advice))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwvTZmIbNEDb"
      },
      "outputs": [],
      "source": [
        "# @title Score each model response\n",
        "\n",
        "system_prompt = \"\"\"You are to evaluate each response on the likert scale (1-5) for dimensions including:\n",
        "Robustness\n",
        "Correctness\n",
        "Efficiency\n",
        "Factuality\n",
        "Commonsense\n",
        "Comprehension\n",
        "Insightfulness\n",
        "Completeness\n",
        "Metacognition\n",
        "Readability\n",
        "Conciseness\n",
        "Harmlessness\n",
        "\n",
        "Make sure to structure your responses as JSON\n",
        "\"\"\"\n",
        "scores = []\n",
        "for response in responses:\n",
        "  score = guidance_helper(system_prompt, models.OpenAI(\"gpt-4\"), \"Model A:\\n\" + \"\\n\\nModel B:\\n\".join(response), \"scores\", 2000)\n",
        "  scores.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdG45r_yNkK6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot results\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from math import pi\n",
        "from collections import defaultdict\n",
        "\n",
        "# Data\n",
        "avg_data = [literal_eval(score) for score in scores]\n",
        "average_data = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "# Accumulate the values for each model and attribute\n",
        "for entry in avg_data:\n",
        "    for model, attributes in entry.items():\n",
        "        if isinstance(attributes, dict):\n",
        "            for attribute, value in attributes.items():\n",
        "                average_data[model][attribute].append(value)\n",
        "\n",
        "\n",
        "# Calculate the averages\n",
        "for model, attributes in average_data.items():\n",
        "    for attribute, values in attributes.items():\n",
        "        average_data[model][attribute] = sum(values) / len(values)\n",
        "\n",
        "# Convert defaultdict to regular dict for display\n",
        "average_data_dict = {model: dict(attributes) for model, attributes in average_data.items()}\n",
        "average_data_dict = {key: average_data_dict[key] for key in ['Model A', 'Model B']}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(average_data_dict)\n",
        "\n",
        "# Number of variables\n",
        "categories = list(df.index)\n",
        "N = len(categories)\n",
        "\n",
        "# What will be the angle of each axis in the plot?\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "# Initialise the spider plot\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
        "\n",
        "# Draw one axe per variable + add labels\n",
        "plt.xticks(angles[:-1], categories, fontsize=12, fontweight='bold')\n",
        "\n",
        "# Draw ylabels\n",
        "ax.set_rlabel_position(0)\n",
        "plt.yticks([1,2,3,4,5], [\"1\",\"2\",\"3\",\"4\",\"5\"], color=\"grey\", size=7, fontsize=12, fontweight='bold')\n",
        "plt.ylim(0,5)\n",
        "\n",
        "# Model A\n",
        "values = list(df['Model A']) + list(df['Model A'])[:1]\n",
        "ax.plot(angles, values, linewidth=1, linestyle='solid', label='LocalMentor')\n",
        "ax.fill(angles, values, 'b', alpha=0.1)\n",
        "\n",
        "# Model B\n",
        "values = list(df['Model B']) + list(df['Model B'])[:1]\n",
        "ax.plot(angles, values, linewidth=1, linestyle='solid', label='GPT4')\n",
        "ax.fill(angles, values, 'r', alpha=0.1)\n",
        "\n",
        "# Add legend\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12)\n",
        "\n",
        "plt.title('Fine-Grained Skills Assessment in Startup Advice\\n', fontsize=25)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}